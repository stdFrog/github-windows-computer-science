가장 흔하고 오래된 문자 코드는 7비트 크기로 영문, 숫자, 기호 등을 대응시킨 ASCII(아스키)코드이다.
지금도 많이 사용될 뿐만 아니라 이후 확장된 문자 코드들도 아스키를 기반으로 한다.

윈도우즈 USER, GDI 등의 모듈이 사용하는 ANSI 문자셋, FAT 파일 시스템이 사용하는 OEM 문자셋도
아스키 문자셋에 기반하되 0x80 이후의 문자가 약간 다르게 정의되어 있다.

ASCII, ANSI 문자셋 얘네를 일반적으로 SBCS(Single Byte Character Set)라고 한다.

여기서 동양 3국 + 러시아 등의 문자를 표현하기 위해 2바이트를 필요로하는
DBCS(Double Byte Character Set) 체계를 사용하게 된다.

이 체계는 현재까지도 계속 사용되고 있다.

현재 우리가 사용하는 DBCS는 영문, 기호, 숫자를 8비트로 표현하고 한글은 16비트로 표현하는 ANSI의 확장형
문자 코드라고 볼 수 있다.

SBCS + DBCS가 섞여 존재하기도 하며 때로는 16비트를 넘을 수도 있기 때문에 MBCS(Multi Byte Character Set)라고 불리기도 한다.

어쨋거나 이런 문자 체계는 다음과 같은 문자를 읽을 때 문자의 길이가 일정하지 않다는 단점이 있다.
> 안녕 Korea

한글은 2바이트, 영문 및 기호는 1바이트만 차지한다.
보다시피 길이가 일정하지 않고 구분하는 것과 바로 앞의 문자를 찾는 등의 동작이 상당히 번거롭고
어렵기 때문에 불편한 점이 많았다.

또한, 코드 페이지에 따라서 실제 맵핑되어 있는 문자가 달라질 수 있다는 큰 문제도 가지고 있었다.
즉, 국제적인 범용 프로그램 제작은 부적당하다는 것이다.




ISO/IEC가 주관한 ISO 10646 국제 표준에서 만들어진 10646 Character Set : 코드 통합을 위해 시행됨 - 역사적으로 최초의 국제적 시도
UCS-4 : 10646의 4바이트를 모두 사용하는 코드
UCS-2 : 10646의 하위 16비트(BMP)만 사용하는 코드

10646이 한창 제정되고 있을 때 애플, 오라클, IBM, 마이크로소프트
유니코드 컨소시엄을 구성했다.

이들은 16비트의 단일화된 코드 체계를 만들고자 했다.

10646 코드는 미래의 모든 문자를 포함하기 위한 이상적인 표준이며
Unicode는 조금 더 현실적인 표준이라고 할 수 있다.

ISO에서 제정한 국제 표준 코드는 10646
유명 대기업들이 제정한 국제 표준 코드는 Unicode

이 둘이 각각 표준을 제정하려고 했었으나 두 그룹으로 나뉘었다는 것에서 이미
단일화된 표준을 만들 수 없으므로 두 그룹이 표준을 통합한다.

10646은 모든 문자에 4바이트 코드를 할당함으로써 세계의 모든 문자를 단일한 코드 체계로 표현하고자 했다.
4바이트의 코드는 각 바이트별로 다음과 같은 구조를 가진다.

[그룹][평면][행][셀]

하위 16비트로 65536개의 문자를 표현하고 이런 문자 그룹을 묶어 하나의 평면(plane)을 구성하고
256개의 평면이 하나의 그룹이 되어 총 127개의 그룹을 정의할 수 있다.
10646은 최상위 1비트는 사용하지 않으며 31비트만 사용하므로 표현할 수 있는 문자의 개수는 20억개에 이른다.
하지만 현실적으로 문자가 이렇게 많지는 않으며 개별 문자마다 4바이트씩을 할당하는 것은 낭비가 심하기 때문에
그룹 0의 평면 0에만 문자 코드를 할당해 놓고 나머지 부분은 예비로 남겨뒀다(예약된 부분도 있다).

그룹 0, 평면 0의 65536문자 코드를 BMP(Basic Multilingual Plane)라고 한다.
이 영역에 알파벳과 한중일 동양의 문자 등이 모두 포함되어 있다.

BMP에는 약 35000자 정도의 코드가 배정되어 있으며 2만여 자는 아직 예비용으로 남아있다.

10646의 BMP가 유니코드 표준과 맞게 수정됨으로써
현재 하위 16비트의 코드 체계는 두 표준이 동일하며

우리가 알고있는 유니코드는 현재 이 통합된 표준을 말하는 것이다.

유니코드는 단일한 문자 코드 체계를 잘 정의하기는 했지만 모든 경우에 이 표준이 적합하지는 않았다.
그래서 실제 적용예에 맞게 변형할 필요가 있었으며 이를 UTF(UCS Transformation format)이라고 한다.

UTF-1, UTF-7, UTF-8, UTF-16, UTF-32 등이 있다.
각 인코딩 방식은 유니코드 문자를 바이트 스트림에 어떻게 배치할 것인가가 다르다.

현실적으로 많이 사용되는 두 가지에 대해서만 알아보자.





UTF-16

	이 형식은 유니코드의 표준대로 모든 문자를 16비트로 표현한다. 그러나 16비트만으로 세계의 문자를 표현하기에는
	조금 부족하며 특히 한자가 그렇다.
	중국인이 사용하는 한자는 대략 55000자 정도이며 이나마도 표준화된 것이 아니다.

	이런 경우를 위해 UTF-16은 서로게이트(Surrogate)라는 확장 코드를 정의함으로써 더 많은 문자를 표현할 수 있게끔
	가능성을 열어 놓았다.

	서로게이트는 16비트 코드의 쌍, 즉 4바이트로 문자 하나를 표현하는데 상위 워드는 110110으로 시작하며,
	하위 워드는 110111로 시작한다.

	즉, 0xd800 ~ 0xdfff까지를 서로게이트 헤더로 사용하는 것이다.
	High Word :		[1] [1] [0] [1] [1] [0] [] [] [] [] [] [] [] [] [] []
	Low Word :		[1] [1] [0] [1] [1] [1]	[] [] [] [] [] [] [] [] [] []
	
	서로게이트 헤더를 제외하면 상위에 10비트, 하위에 10비트가 남아 총 20비트 크기의 문자를 표현할 수 있다.
	즉, 백만이 넘는 문자를 표현할 수 있으며 윈도우즈는 운영체제 차원에서 서로게이트 문자를 지원해준다.

	GDI의 폰트 포맷, 텍스트 출력함수, USER 모듈, 에디트, 리치 에디트 컨트롤 등이 모두 서로게이트를 인식한다.

	그러나 모든 시스템 요소가 서로게이트를 완벽하게 지원하는 것은 아니며 개발자가 유니코드를 직접 다룰 때
	서로게이트를 염두에 두어야 한다.

	서로게이트가 존재함으로써 UTF-16은 사실상 고정 길이가 아니라 가변 길이를 가지는 문자 코드라 볼 수 있다.
	그러나 서로게이트가 꼭 필요한 경우는 많지 않기 때문에 대개의 경우 큰 문제없이 2바이트만 사용한다.



UTF-8

	유니코드는 국제화가 쉽고 동시에 다양한 문자를 표현할 수 있다는 면에서 긍정적인 코드 체계이다.
	동양 3국의 입장에서 볼 때 유니코드는 그럭저럭 만족할만한 포맷이다.

	그러나 영어권 사람들 입장에서 볼 때 유니코드는 메모리를 두 배나 잡아먹는 얄미운 포맷이다.
	안시로 저장하면 1K크기인 것이 2K로 늘어나 버리며 게다가 지금까지 잘 써 왔던 아스키 코드와도
	호환되지 않으므로 이따위 포맷이 마냥 반갑지는 않을 것이다.

	그래서 UTF-8은 이러한 단점을 해결하고자 만들어진 인코딩 형식이다.
	UTF-8은 가변 길이의 체게를 가지며 표현하고자 하는 코드에 따라 다음과 같이 여러 바이트를 사용한다.

	0x1 ~ 0x7f		[0][ ][ ][ ][ ][ ][ ][ ]	
	0x80 ~ 0x7ff	[1][1][0][ ][ ][ ][ ][ ]	[1][0][ ][ ][ ][ ][ ][ ]
	0x800 ~ 0xffff	[1][1][1][0][ ][ ][ ][ ]	[1][0][ ][ ][ ][ ][ ][ ]	[1][0][ ][ ][ ][ ][ ][ ]
	0x10000 ~		[1][1][1][1][0][ ][ ][ ]	[1][0][ ][ ][ ][ ][ ][ ]	[1][0][ ][ ][ ][ ][ ][ ]	[1][0]...

	0x80이상의 코드는 첫 바이트의 선두에 110을 넣고 두 번째 바이트의 선두에 10을 넣은 후
	양쪽의 남은 11비트에 문자 코드를 기록한다. 따라서 0x7ff까지 코드를 표현할 수 있다.

	이와 같은 방식으로 0xffff까지는 선두에 1110을, 그리고 이어지는 두 바이트에 10을 두고 남은
	16비트에 코드를 기록한다.
	물론 16비트보다 더 긴 코드도 4바이트 이상을 사용하면 얼마든지 표현할 수 있다.

	이런 식이면 영문과 숫자는 1바이트로 표현할 수 있어서 영어권의 문자를 별다른 낭비없이 표현할 수 있다.
	그러나, 동양 3국의 문자는 3바이트가 필요하기 때문에 UTF-16보다는 더 많은 메모리가 필요하다.

	똑같은 문자열을 UTF-8과 UTF-16으로 표현할 때 어느쪽이 더 짧을 것인가는 문자열내의 글자 구성에 따라 달라진다.

	알파벳이나 숫자가 많으면 UTF-8이 짧고 한글이 많으면 UTF-16이 더 짧다.
	그러나 압축 기술이 발전한 요즘은 이런 길이를 비교하는 것이 사실 큰 의미가 없다.

	"A a 한8"이라는 글자가 인코딩되는 방식을 비교해 보자.

				 A		 a		 한		 8
	ANSI	:	41		A5E7	C7D1	38
	UTF-16	:	0041	03B1	D5DC	0038
	UTF-8	:	41		CEB1	ED959C	38


	안시 인코딩에서 한글은 완성형 코드로 표현된다. 유니코드는 0x7f이전까지만 아스키와 호환되므로
	한글이나 특수문자는 완전히 다른 코드로 표현된다.

	각 문자의 코드값은 문자표에서 찾아볼 수 있다. UTF-8은 유니코드 문자를 다음과 같이 인코딩한다.

						a							 한
	UTF-16	:	000000[11 10][110001]		[1101][0101		01][011100]
	UTF-8	:	1100[1110] 10[110001]		1110[1101]		10[010101]	10[011100]
	

	0x7ff이하의 11비트 코드는 헤더를 제외한 상위에 5비트, 하위에 6비트를 나누어 넣는다.
	0x800이상의 16비트 코드는 헤더를 포함하여 3바이트에 나누어 저장하되 각 바이트에 4, 6, 6비트씩을 저장한 것이다.

	문자를 표현하는 코드는 UTF-16과 동일하되 코드의 각 비트를 나누어 저장하는 방식만 다르다.

	UTF-8이 우수한 이유는 단지 아스키 코드와 호환된다는 점 때문만은 아니며 여러 장점이 존재한다.
	1. 문자 코드의 길이를 계산하기 쉽다. 비록 가변 길이이지만 상위 비트를 검사하면 문자의 길이를 바로 알 수 있다.
	2. 문자의 선두를 찾는 것도 아주 간단하다. 선두 바이트가 아닌 이어지는 바이트는 항상 10으로 시작된다.
	3. 특정 문자의 코드가 다른 문자의 부분 코드로 나타나지 않아 부분 문자열 검색 알고리즘이 단순하고 빠르다.
		A의 코드인 0x41이 한글이나 특수 문자의 허리 부분에서 발견되는 일이 절대로 없다는 것이다.
		그래서 원하는 코드를 바이트 검색하기만 하면 정확한 문자를 찾을 수 있다.

	UTF-8은 인간보다는 기계가 처리하기 쉽게 만들어졌으며 헤더를 제외한 문자 코드를 추출하기 위해
	쉬프트와 OR 정도의 간단한 연산만 필요하여 굉장히 빠르다.




바이트 순서

	유니코드는 어떤 숫자에 어떤 문자를 대응시킬 것인가는 정확하게 규정하고 있지만
	바이트 순서에 대해서는 규정이 없다.

	2바이트 이상의 값을 표현하는 방식에는 빅 엔디안, 리틀 엔디안 두 가지 방식이 있는데
	유니코드는 이 두 가지 방식으로 모두 표현된다.

	그러다보니 어떤 바이트 순서로 되어 있는지를 표시할 수 있는 방법이 필요해졌으며 이런 목적으로
	유니코드의 문서 선두에는 바이트 순서를 나타내는 특별한 표식 BOM(Byte Order Mark)이 배치된다.

	BOM은 제어 코드가 아니다.
	단지 응용 프로그램에게 문서의 해석 방법에 대한 정보만 제공할 뿐이다.

	만약 문서의 바이트 순서가 프로세서의 고유한 바이트 순서와 다르다면 코드를 읽을 때 뒤집어 읽어야 한다.
	이때 이 BOM을 확인하여 어떻게 읽을 것인가를 프로그래머가 미리 알 수 있게끔 한다.


	BOM				설명
	
	EF BB BF		UTF-8
	FF FE			UTF-16, BE
	FE FF			UTF-16, LE			// 비교할때 이대로 쓰면 됨 0xFEFF
	FF FE 00 00		UTF-32, LE
	FE FF 00 00		UTF-32, BE

	
	BOM은 원래 바이트 순서를 표시하기 위해 붙이는 것이지만,
	그 자체로 유니코드 문서에 대한 매직 넘버로 활용되기도 한다.

	즉, FE FF로 시작했다면 UTF-16 문서라는 것을 확실히 알 수 있다.
	FE, FF등의 코드에는 문자가 전혀 할당되어 있지 않으므로 정상적인 문서의 중간에 나올 수 없으며
	나온다 하더라도 무시된다

	이중 UTF-16 LE가 마이크로소프트의 표준이며 원한다면 다른 방식의 문서를 변환하거나 읽고 쓸 수 있다.

	"1한A"라는 짧은 문자열을 각각의 방식으로 저장했을 때 실제 어떻게 저장되는지 헥사 에디터로 조사해 보면 다음과 같다.
	
	인코딩				데이터

	ANSI				31 C7 D1 41
	Unicode				FF FE 31 00 5C D5 41 00		// LE 헥스값으로 봤을 때
	Unicode(BE)			FE FF 00 31 D5 5C 00 41
	UTF-8				EF BB BF 31 ED 95 9C 41		// 얘는 고유의 순서가 있음
													// 엔디안 영향 안받음

	안시로 저장할 때는 빅엔디안의 한글 완성형으로 저장된다.
	별도의 BOM은 붙지 않으며 유니코드로 저장했을 때만 유효하다.

	LE는 0xFEFF로 BOM을 지정했을 때 헥스 에디터로 읽으면
	FF FE순서 즉, 하위 바이트가 먼저 오며 BE는 유니코드가 순서대로 나타난다.

	조금 특이한게 UTF-8은 인코딩 방식 자체에 순서 규정이 있기 때문에 엔디안의 영향을 받지 않는다.
